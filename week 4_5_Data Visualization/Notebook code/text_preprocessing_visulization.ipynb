{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cZRse7nlUZJx",
        "-QFxUcQug_1i"
      ],
      "toc_visible": true,
      "mount_file_id": "1TKyHwpD2goA47n-N8QG6hqG7E0IdwVTh",
      "authorship_tag": "ABX9TyNAIytWBl/g+sZxmS9zkIr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakarla/AI-in-the-Built-Environment/blob/main/week%204_5_Data%20Visualization/Notebook%20code/text_preprocessing_visulization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text pre-processing and visulization:**\n",
        "In this section, we will implement several preprocessing techniques on the text data collected previously. The objective of this part is to familiarize you with essential steps that enhance data clarity, ensuring it is well-prepared for the machine learning process.\n",
        "\n",
        "\n",
        "*    Learn text preprocessing\n",
        "*    Visualization\n",
        "  *   Bar chart\n",
        "  *   Cloud of words\n",
        "*  Bag-of-words\n",
        "*  Word2Vec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "m058kdTZGfaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Text preprocessing**"
      ],
      "metadata": {
        "id": "-GoEaR04kK3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start, we will link this notebook to your Google Drive. Make sure you are logged in on your Google account"
      ],
      "metadata": {
        "id": "6X58uBbikqpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "mX5h6fSwkqP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's combine all the data we've collected to create one file.\n",
        "\n",
        "You can change the directory to where your files are! (*```folder_path = '/content/drive/MyDrive/Colab Notebooks/text'```*)\n",
        "Also, you can change the directory where you want to save a new file. (*``` with open('/content/drive/MyDrive/Colab Notebooks/text/combined_data.json', 'w') as f:```*)"
      ],
      "metadata": {
        "id": "cBYlYj7eGqmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "#change directory if you need\n",
        "##################################################################\n",
        "# Specify the folder containing your JSON files\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/text'\n",
        "##################################################################\n",
        "\n",
        "# List all JSON files in the folder\n",
        "json_files = [file for file in os.listdir(folder_path) if file.endswith('.json')]\n",
        "\n",
        "combined_data = []\n",
        "\n",
        "for file in json_files:\n",
        "    with open(os.path.join(folder_path, file), 'r') as f:\n",
        "        data = json.load(f)\n",
        "        combined_data.extend(data)  # or use `.append(data)` for dictionary data\n",
        "\n",
        "#change directory if you need\n",
        "##################################################################\n",
        "# Save the combined data to a new JSON file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/text/combined_data.json', 'w') as f:\n",
        "    json.dump(combined_data, f, indent=4)\n",
        "##################################################################\n"
      ],
      "metadata": {
        "id": "EhkT6wtdGpsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.1: Import Libraries**\n",
        "\n",
        "Make sure to install any libraries that are not already installed by using `!pip install library_name`."
      ],
      "metadata": {
        "id": "1NNfNmjAHlwG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkqXNsZkF6Yx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.2: Load text File**\n",
        "\n",
        "Assuming you have a JSON file called **combined_data.json**, you can read it as follows:"
      ],
      "metadata": {
        "id": "OymXhlgCIuzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "###########################################################################\n",
        "# Load JSON Data\n",
        "json_file = '/content/drive/MyDrive/Colab Notebooks/text/combined_data.json'  # Replace with the path to your JSON file\n",
        "###########################################################################\n",
        "\n",
        "with open(json_file, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "lC7j-BVJyzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.3: Extract and Preprocess Text Data**\n",
        "\n",
        "We will extract the text content from the JSON file and preprocess it."
      ],
      "metadata": {
        "id": "lvU0J5qSfPtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store text content\n",
        "text_data = []\n",
        "#  Extract \"content\" from JSON\n",
        "for item in data:\n",
        "    if 'content' in item:\n",
        "        text_content = item['content']\n",
        "\n",
        "        # Join the list of content elements into a single string\n",
        "        text_content = ' '.join(text_content)\n",
        "        text_data.append(text_content)\n"
      ],
      "metadata": {
        "id": "o5LuPimmzCE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, **text_data** contains a list of sentences from your text file."
      ],
      "metadata": {
        "id": "72yDoZHJqPSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Before preprocessing, it's essential to understand your data. You can do this by checking the first few rows of your Data and getting some basic statistics."
      ],
      "metadata": {
        "id": "WzPxCZQQgqgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few sentences\n",
        "print(text_data[:1])  # Replace 5 with the number of sentences you want to display\n",
        "\n",
        "# Get basic statistics of the text data\n",
        "print(\"Number of text:\", len(text_data))\n"
      ],
      "metadata": {
        "id": "MF4nHs4-fVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1.4: Text Cleaning**\n",
        "\n",
        "Text data often contains noise that needs to be cleaned. Here are some common text cleaning steps:\n"
      ],
      "metadata": {
        "id": "FF2tHWPkfZ4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lowercasing**: Convert all text to lowercase to ensure consistency."
      ],
      "metadata": {
        "id": "YjxTJB_rrPTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_data)"
      ],
      "metadata": {
        "id": "sPMnVqwy7M_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [sentence.lower() for sentence in text_data]\n",
        "print(text_data)"
      ],
      "metadata": {
        "id": "9SW3ThFjfcx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Special Characters and Numbers:** Remove punctuation, special characters, and numbers using regular expressions.\n",
        "\n",
        "`sentence.replace('\\n', ' ')`: Within each sentence, this part of the code uses the replace method to replace all occurrences of the newline character ('\\n') with nothing('').\n",
        "\n",
        "`r'[^a-zA-Z\\s]':` This regular expression pattern matches any character that is not an uppercase or lowercase alphabet letter (a-zA-Z) and not a whitespace character (\\s)."
      ],
      "metadata": {
        "id": "KoWfk9asfish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [re.sub(r'[^a-zA-Z\\s]', '', sentence.replace('\\n', '')) for sentence in text_data]\n",
        "\n",
        "print(text_data)"
      ],
      "metadata": {
        "id": "il45ZMn5fiTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization:** Split text into individual words or tokens."
      ],
      "metadata": {
        "id": "sfJHY1Refuus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "gFS_JgFqxpD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = [nltk.word_tokenize(sentence) for sentence in text_data]\n",
        "print(text_data)"
      ],
      "metadata": {
        "id": "laPiGoOifw0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopword Removal:** Remove common stopwords (e.g., \"the\", \"and\", \"is\") to reduce noise."
      ],
      "metadata": {
        "id": "F-cf2A42hQis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_no_stopwords = [[word for word in sentence if word not in stop_words] for sentence in text_data]"
      ],
      "metadata": {
        "id": "2DxQFj-YyGNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_no_stopwords)"
      ],
      "metadata": {
        "id": "ek9TjLmUyKa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Visualization**:\n",
        "We want to visualize the frequency of words step by step. you can create a bar chart to display the word frequencies."
      ],
      "metadata": {
        "id": "oaRanMYy1FlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.1 bar chart**"
      ],
      "metadata": {
        "id": "8B3kBy9KipY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.1:Import Libraries**\n",
        "\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "f1X68Sgv1UOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RlVg3tDh1EhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.2**: Count Word Frequencies\n",
        "\n",
        "Count the frequency of each word in the preprocessed text data:\n",
        "\n"
      ],
      "metadata": {
        "id": "cQ-iXx8j1a78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all preprocessed sentences into a single list of words\n",
        "all_words = [word for sentence in text_data for word in sentence]\n",
        "\n",
        "# Count word frequencies\n",
        "word_freq = Counter(all_words)\n"
      ],
      "metadata": {
        "id": "3uv7bhQQ1Ed4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq"
      ],
      "metadata": {
        "id": "FLqqTBtN8VuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2.3**: Visualize Word Frequencies\n",
        "\n",
        "Visualize the word frequencies using a bar chart:"
      ],
      "metadata": {
        "id": "XZ92qqxB1icl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the top N most common words\n",
        "N = 20  # Change this value to display more or fewer words\n",
        "most_common_words = word_freq.most_common(N)\n",
        "\n",
        "# Extract the words and their frequencies\n",
        "words, frequencies = zip(*most_common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(words, frequencies)\n",
        "plt.title(f\"Top {N} Most Common Words\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dP3-XzZ41Ea-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will count the frequencies of words in the preprocessed text data and create a bar chart to visualize the top N most common words. You can adjust the value of N to display more or fewer words in the chart."
      ],
      "metadata": {
        "id": "ORej1c0s3sUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.2 Create Cloud words:**\n",
        "\n",
        "**Step 1:** Import Libraries\n",
        "\n",
        "Import the necessary libraries:"
      ],
      "metadata": {
        "id": "Ozz1JX0PzTaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "jNXXVfGbzS0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Combine Preprocessed Text\n",
        "\n",
        "Combine all the preprocessed sentences into a single text:"
      ],
      "metadata": {
        "id": "ue-nD1tkzqgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_text = \" \".join([\" \".join(sentence) for sentence in text_data])\n",
        "print(combined_text)"
      ],
      "metadata": {
        "id": "Xs6VOggszSwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Generate the Word Cloud\n",
        "\n",
        "Generate the word cloud from the combined text:"
      ],
      "metadata": {
        "id": "OhUOgCfEz1Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n"
      ],
      "metadata": {
        "id": "6vuSjipszStP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Display the Word Cloud\n",
        "\n",
        "Display the word cloud using matplotlib:"
      ],
      "metadata": {
        "id": "c1ju_RqTz8X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FTv-_cldzSog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Bag- of-words**"
      ],
      "metadata": {
        "id": "cZRse7nlUZJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words is a simple text representation method where each document is represented by a fixed-length vector. Each element of the vector corresponds to the frequency of a word in the document."
      ],
      "metadata": {
        "id": "KjM5koxRf76q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Flatten the token list into strings for vectorization\n",
        "flattened_text = [' '.join(token) for token in tokens_no_stopwords]\n",
        "\n",
        "# create the vocabulary\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# fit the vocabulary to the text data\n",
        "vectorizer.fit(flattened_text)\n",
        "\n",
        "# create the bag-of-words model\n",
        "bow_model = vectorizer.transform(flattened_text)\n",
        "\n",
        "# print the bag-of-words model\n",
        "print(bow_model)"
      ],
      "metadata": {
        "id": "ervXxlIkUbkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bow_model variable is a sparse matrix that contains the frequency of each word in the vocabulary for each text document in the *text_data list*. You can access the vocabulary and the mapping from words to indices using the *vocabulary_ attribute* of the *CountVectorizer* object."
      ],
      "metadata": {
        "id": "fPafNyp-gSFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the vocabulary\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# print the word-to-index mapping\n",
        "print(vectorizer.vocabulary_['images'])"
      ],
      "metadata": {
        "id": "iI7D45l6UsBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Alternatives to a Bag-of-Words in Python**\n",
        "\n",
        "When representing text data in natural language processing (NLP) tasks, several alternatives to the bag-of-words model can be more effective:\n",
        "\n",
        "### 1. N-grams\n",
        "- **Definition**: Contiguous sequences of n words in a text document.\n",
        "- **Usefulness**: Captures the relationship between adjacent words, helpful for understanding word order and meaning.\n",
        "\n",
        "### 2. Word Embeddings\n",
        "- **Definition**: Dense, low-dimensional representations of words.\n",
        "- **Usefulness**: Captures the semantic relationships between words, representing their meaning and relationships.\n",
        "\n",
        "### 3. Part-of-Speech Tags\n",
        "- **Definition**: Identifies the part of speech (e.g., noun, verb, adjective) of each word.\n",
        "- **Usefulness**: Captures the syntactic structure and relationships between words.\n",
        "\n",
        "### 4. Named Entity Recognition (NER)\n",
        "- **Definition**: Identifies and classifies named entities (e.g., people, organizations, locations) in a text document.\n",
        "- **Usefulness**: Extracts structured information from unstructured text, identifying important entities.\n",
        "\n",
        "### 5. Syntactic Parsing\n",
        "- **Definition**: Analyzes the structure of a sentence and determines relationships between words.\n",
        "- **Usefulness**: Captures the syntactic structure and relationships between words in a text document.\n"
      ],
      "metadata": {
        "id": "QnaN_vD4mv2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Word2Vec**\n",
        "**Word2Vec** is a **word embedding** technique that represents words in continuous vector space where semantically similar words are mapped to nearby points."
      ],
      "metadata": {
        "id": "-QFxUcQug_1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(tokens_no_stopwords, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get the vector for a word\n",
        "word_vector = word2vec_model.wv['house']\n",
        "print(\"Word vector for 'house':\", word_vector)\n",
        "\n",
        "# Check similarity between two words\n",
        "similarity = word2vec_model.wv.similarity('house', 'home')\n",
        "print(\"Similarity between 'house' and 'home':\", similarity)"
      ],
      "metadata": {
        "id": "4yNDgVQKU9m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xkFQBtJRkchX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}